<!DOCTYPE html>
<html>
  <title>Olivia's Research</title>
  <link rel="stylesheet" href="main.css" />
  <body a="auto">
    <main class="page-content" aria-label="Content">
        <div class="w">
            <a href="/"></a><h1>&lt;research papers&gt;</h1><br>
            <a href="https://long-olivia.github.io/">[home]</a>
            <p>A little something I coauthored. More to come!</p>
            <h2><a href="https://arxiv.org/abs/2506.21561" target="_blank">"Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs"</a><br></h2>
            <p>Emilio Barkett, Olivia Long, Madhavendra Thakur</p>
            <p>06.12.2025</p>
            <p>Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models. We find that rates of truth-bias, or the likelihood to believe a statement is true, regardless of whether it is actually true, are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. Most concerning, we identify sycophantic tendencies in several advanced models (o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy. This suggests that capability advances alone do not resolve fundamental veracity detection challenges in LLMs.</p>
        </div>
    </main>
</body>
</html>